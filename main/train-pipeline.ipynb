{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c3381b-1ec0-4604-86c3-94c1789924c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models, transforms\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join as join_path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1783f2fe-795f-4b5c-a283-ad3e1f2a2c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget http://images.cocodataset.org/zips/val2017.zip\n",
    "# !unzip val2017.zip > /dev/null\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201d8c4-7ae9-42d8-9b1f-90e2cd836235",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c627c7-7d25-4383-9a90-ac5d79bb03d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_TO_FOLDER = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95ca25c2-2282-4efe-b837-0d5feee5d5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(join_path(PATH_TO_FOLDER, 'train2017.csv'))\n",
    "val_df = pd.read_csv(join_path(PATH_TO_FOLDER, 'val2017.csv'))\n",
    "test_df = pd.read_csv(join_path(PATH_TO_FOLDER, 'test2017.csv'))\n",
    "\n",
    "train_df['img_dir'] = join_path(PATH_TO_FOLDER, 'train2017')\n",
    "val_df['img_dir'] = join_path(PATH_TO_FOLDER, 'val2017')\n",
    "test_df['img_dir'] = join_path(PATH_TO_FOLDER, 'test2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43b16e4b-435e-462f-97e0-9a428d58e842",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 400 ms, sys: 0 ns, total: 400 ms\n",
      "Wall time: 398 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def calc_metrics(df, eps=1e-5):\n",
    "    metrics = list(df.metric.value_counts().index)\n",
    "    df['before_score_norm'] = 0\n",
    "    df['after_score_norm'] = 0\n",
    "    df['gain_percentage_norm'] = 0\n",
    "    for metric in metrics:\n",
    "        _min, _max = min(df[df['metric'] == metric]['before_score'].min(), df[df['metric'] == metric]['after_score'].min()), max(df[df['metric'] == metric]['before_score'].max(), df[df['metric'] == metric]['after_score'].max())\n",
    "        df.loc[df['metric'] == metric, 'before_score_norm'] = (df.loc[df['metric'] == metric, 'before_score'] - _min) / (_max - _min)\n",
    "        df.loc[df['metric'] == metric, 'after_score_norm'] = (df.loc[df['metric'] == metric, 'after_score'] - _min) / (_max - _min)\n",
    "    df['gain_percentage_norm'] = (df['after_score_norm'] - df['before_score_norm']) / (df['before_score_norm'] + eps)\n",
    "    return df\n",
    "\n",
    "train_dff = calc_metrics(train_df)\n",
    "val_df = calc_metrics(val_df)\n",
    "test_df = calc_metrics(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1ce2eb9-6dfb-4cc6-b75f-6bf837d30cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_slice_df(df, metric_name=None, attack_type=None):\n",
    "    if metric_name and attack_type:\n",
    "        return df[(df['metric'] == metric_name) & (df['attack_type'] == attack_type)]\n",
    "    elif metric_name:\n",
    "        return df[(df['metric'] == metric_name)]\n",
    "    elif attack_type:\n",
    "        return df[(df['metric'] == metric_name)]\n",
    "    return\n",
    "\n",
    "class ImageGainDataset(Dataset):\n",
    "    def __init__(self, images: list,  target: list, transforms=None):\n",
    "        self.images = images\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.target = target\n",
    "        assert len(self.images) == len(self.target), \"Wrong len\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.images[idx]\n",
    "        image = Image.open(filename).convert('RGB')\n",
    "        image_np = np.array(image)\n",
    "        if len(image_np.shape) == 2:\n",
    "            image_np = image_np[:, :, None]\n",
    "            image_np = np.concatenate((image_np, image_np, image_np), axis=2)\n",
    "            image = Image.fromarray(image_np)\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        target = self.target[idx]\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d1fa6ea0-d254-4aeb-b50f-136142295dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_target(df, metric_name: str, attack_name: str, quantiles: List[float], gain_name: str, qs_calculated=None):\n",
    "    \"\"\"Сначала выделяем нулевой класс, затем считаем квантили.\"\"\"\n",
    "    target = pd.Series(0, index=sorted(list(set(df['img_dir'] + '/' + df['image_name']))), name='target')\n",
    "    x = get_slice_df(df, metric_name, attack_name)\n",
    "    index = x['img_dir'] + '/' + x['image_name']\n",
    "    data = x[gain_name]\n",
    "    ind = index[data[data < 0].index]\n",
    "    target[ind] = 0\n",
    "    data = data[data > 0]\n",
    "    qs = []\n",
    "    for i in range(1, len(quantiles)):\n",
    "        q_left = quantiles[i-1]\n",
    "        q_right = quantiles[i]\n",
    "        if qs_calculated is not None:\n",
    "            q1, q2 = qs_calculated[i-1][1:]\n",
    "        else:\n",
    "            q1 = data.quantile(q_left)\n",
    "            q2 = data.quantile(q_right)\n",
    "        \n",
    "        if i == len(quantiles) - 1:\n",
    "            ind = index[data[(data >= q1) & (data <= q2)].index]\n",
    "        else:\n",
    "            ind = index[data[(data >= q1) & (data < q2)].index]\n",
    "        qs.append([i, q1, q2])\n",
    "        target[ind] = i\n",
    "    return target, qs\n",
    "\n",
    "\n",
    "def create_mos_target(df, metric_attack_dict, quantiles: List[float], gain_name='gain_percentage_norm', qs_calculated=None):\n",
    "    targets = pd.DataFrame(index=sorted(list(set(df['img_dir'] + '/' + df['image_name']))))\n",
    "    qs = {}\n",
    "    for metric_name in metric_attack_dict.keys():\n",
    "        for attack_name in metric_attack_dict[metric_name]:\n",
    "            if qs_calculated is not None:\n",
    "                x = qs_calculated[f'{metric_name}_{attack_name}']\n",
    "            else:\n",
    "                x = None\n",
    "            targets[f'{metric_name}_{attack_name}'], xx = create_target(df, metric_name, attack_name, quantiles, gain_name, x)\n",
    "            qs[f'{metric_name}_{attack_name}'] = xx\n",
    "    return targets, qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "29401d18-c39e-48e0-b5f8-30a896a2eecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_attack_dict = {\n",
    "    'koncept': ['cnn_24_255', 'fgsm_2_255', 'ifgsm_alpha3_eps2_i3', 'uap_26_255'],\n",
    "    'lin': ['cnn_22_255', 'fgsm_3_255', 'ifgsm_alpha1_eps3_i2', 'uap_20_255'],\n",
    "    'p2p': ['cnn_8_255', 'fgsm_5_255', 'ifgsm_alpha2_eps5_i3', 'uap_10_255'],\n",
    "    'spaq': ['cnn_40_255', 'fgsm_5_255', 'ifgsm_alpha1_eps3_i2', 'uap_20_255'],\n",
    "    'mdtvsfa': ['fgsm_2_255', 'ifgsm_alpha2_eps2_i3', 'uap_9_255'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5297d16-e54f-4715-b6fc-6b1bc27c1689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUANTILES = [0, .25, .75, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4542ffa-63fa-461c-a63b-f042de19c8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_binned_train, _ = create_mos_target(train_df, metric_attack_dict, QUANTILES)\n",
    "target_train = target_binned_train.mean(1)\n",
    "target_binned_val, _ = create_mos_target(val_df, metric_attack_dict, QUANTILES)\n",
    "target_val = target_binned_val.mean(1)\n",
    "target_binned_test, _ = create_mos_target(test_df, metric_attack_dict, QUANTILES)\n",
    "target_test = target_binned_test.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c38c3343-bad1-461f-b523-eb177b74dbaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(26, 4))\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.hist(target_train, bins=100, label=\"train\");\n",
    "# plt.legend();\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.hist(target_val, bins=100, label=\"val\");\n",
    "# plt.legend();\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.hist(target_test, bins=100, label=\"test\");\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5101c42-5a23-4172-9010-60b1e2695eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_mean = target_train.mean()\n",
    "_std = target_train.std()\n",
    "print(_mean, _std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eea6883f-5790-434c-b267-ea22bb0aa43b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = (sorted(list(set(train_df['img_dir'] + '/' + train_df['image_name'])))) # sorted(list(set(train_df['image_name'])))\n",
    "y_train = (target_train[X_train].values - _mean) / _std\n",
    "\n",
    "X_val = (sorted(list(set(val_df['img_dir'] + '/' + val_df['image_name'])))) # sorted(list(set(val_df['image_name'])))\n",
    "y_val = (target_val[X_val].values - _mean) / _std\n",
    "\n",
    "X_test = (sorted(list(set(test_df['img_dir'] + '/' + test_df['image_name'])))) # sorted(list(set(test_df['image_name'])))\n",
    "y_test = target_test[X_test].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c01cbcf-0f67-4732-a9da-b849550b0796",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d51b924c-f0d1-4aaa-94ce-c9cc5ebc3d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    '''Takes in a module and initializes all linear layers with weight\n",
    "       values taken from a normal distribution.'''\n",
    "\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1: # for every Linear layer in a model\n",
    "        y = m.in_features\n",
    "        m.weight.data.normal_(0.0,1/np.sqrt(y)) # m.weight.data shoud be taken from a normal distribution\n",
    "        m.bias.data.fill_(0) # m.bias.data should be 0\n",
    "        \n",
    "def get_n_params(model):\n",
    "    pp = 0\n",
    "    for p in list(model.parameters()):\n",
    "        nn = 1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn * s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "334d8459-3419-48a6-baa4-67f9622f1001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import pad\n",
    "\n",
    "\n",
    "def collate_padding(batch):\n",
    "    h = max([b[0].shape[1] for b in batch])\n",
    "    w = max([b[0].shape[2] for b in batch])\n",
    "    imgs = torch.zeros(len(batch), 3, h, w)\n",
    "    targets = torch.zeros(len(batch))\n",
    "    for i, (im, target) in enumerate(batch):\n",
    "        new_w = int((w - im.shape[2]) / 2)\n",
    "        new_h = int((h - im.shape[1]) / 2)\n",
    "        imgs[i] = pad(im, pad=(new_w, w - new_w - im.shape[2], new_h, h - new_h - im.shape[1]), value=0)\n",
    "        targets[i] = target\n",
    "    return imgs, targets\n",
    "\n",
    "transforms_super_simple = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5, 0.5, 0.5], [0.25, 0.25, 0.25])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22ab47-5447-4511-af78-697f44278a22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## IRAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92698273-8330-4f0e-852a-97793ffc24f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, bias=False) # verify bias false\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, pool_kernel_size=None):\n",
    "        super(CNNBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1), bias=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1), bias=True)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d((pool_kernel_size, pool_kernel_size)) if pool_kernel_size is not None else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IRAA(nn.Module):\n",
    "    def __init__(self, n_classes=1, dropout=0.05, pool_kernel_size=None, average_pool_size=1):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            CNNBlock(3, 64, pool_kernel_size=pool_kernel_size),\n",
    "            nn.Dropout2d(dropout),\n",
    "            CNNBlock(64, 32, pool_kernel_size=pool_kernel_size),\n",
    "            nn.Dropout2d(dropout),\n",
    "            CNNBlock(32, 16, pool_kernel_size=pool_kernel_size),\n",
    "            nn.AdaptiveAvgPool2d((average_pool_size, average_pool_size)),\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16 * average_pool_size * average_pool_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_classes),\n",
    "            nn.Sigmoid()       \n",
    "        )\n",
    "        self.encoder.apply(weights_init_normal)\n",
    "        self.decoder.apply(weights_init_normal)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.decoder(x)\n",
    "        return x * 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f13831-bd16-48bb-b591-ac7a950408b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ResNet18, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8117b9e7-b0e2-447a-b7ec-099fdcf69d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet18Model(nn.Module):\n",
    "    def __init__(self, n_classes=1, pretrained=False):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.encoder = models.resnet18(weights='DEFAULT')\n",
    "        else:\n",
    "            self.encoder = models.resnet18(weights=None)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1000, n_classes),\n",
    "            nn.Sigmoid()   \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x * 3\n",
    "    \n",
    "class ResNet50Model(nn.Module):\n",
    "    def __init__(self, n_classes=1, pretrained=False):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.encoder = models.resnet50(weights='DEFAULT')\n",
    "        else:\n",
    "            self.encoder = models.resnet50(weights=None)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1000, n_classes),\n",
    "            nn.Sigmoid()   \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x * 3\n",
    "    \n",
    "        \n",
    "transform_resnet = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a11866-760d-4060-bf79-1bf24af6badd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## VGG11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49f309b9-8904-4720-aec6-840d88bf581c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VGG11(nn.Module):\n",
    "    def __init__(self, n_classes=1, pretrained=False):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.encoder = models.vgg11(weights='DEFAULT')\n",
    "        else:\n",
    "            self.encoder = models.vgg11(weights=None)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, n_classes),\n",
    "            nn.Sigmoid()   \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eeca5d-f661-4875-88ed-4df1af4f596d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inception-V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5701048-b215-4e8a-90e9-101f071a25a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Inception3Model(nn.Module):\n",
    "    def __init__(self, n_classes=1, pretrained=False):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.encoder = models.inception_v3(weights='DEFAULT')\n",
    "        else:\n",
    "            self.encoder = models.inception_v3(weights=None)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, n_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            x, _ = self.encoder(x)\n",
    "        else:\n",
    "            x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x * 3\n",
    "\n",
    "transform_inception = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f457416-3024-4e5e-8562-bdbb425270a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## EfficientNet-B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "111bd0e0-35d1-42cd-9926-4e4ac8932700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EfficientNetB2(nn.Module):\n",
    "    def __init__(self, n_classes=1, pretrained=False):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.encoder = models.efficientnet_b2(weights='DEFAULT')\n",
    "        else:\n",
    "            self.encoder = models.efficientnet_b2(weights=None)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, n_classes),\n",
    "            nn.Sigmoid()   \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f72a9-79f1-4c91-8a28-32a1a587a2e2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d13294a-b3cd-486b-a8e6-2a491d0756d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ed263709-303c-47f0-9387-4951d2ab986a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_pred_scores = []\n",
    "    all_true_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, total=len(dataloader), desc=\"eval\"):\n",
    "            batch_images, batch_labels = batch\n",
    "            pred_scores = model(batch_images.to(device)).detach().cpu()\n",
    "            all_pred_scores.extend(pred_scores.numpy())\n",
    "            all_true_scores.extend(batch_labels.numpy())\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    all_pred_scores = np.array(all_pred_scores).flatten()\n",
    "    all_true_scores = np.array(all_true_scores).flatten()  \n",
    "    return all_true_scores, all_pred_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c966b3c-a1fd-4bf1-a4bc-dd4a26846773",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8712afc4-8be1-4057-b8b5-22713949d9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = IRAA(dropout=0.1, pool_kernel_size=3, average_pool_size=3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f0993-e140-4ed9-b3f1-fed2abd53ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_model_path = 'iraa.pt'\n",
    "# model.load_state_dict(torch.load(save_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82771d4e-70f6-4b02-bebd-1c7682334e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collate_fn = collate_padding\n",
    "# collate_fn = None\n",
    "current_transforms = transforms_super_simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1b1e723-9667-461b-8d5c-50537991524e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = ImageGainDataset(X_train, y_train, transforms=current_transforms) # transform_resnet, transforms_simple\n",
    "val_dataset = ImageGainDataset(X_val, y_val, transforms=current_transforms)\n",
    "test_dataset = ImageGainDataset(X_test, y_test, transforms=current_transforms)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=1, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=1, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ac5ba7a-ad82-4fe6-bfd4-c349769b775f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = ImageGainDataset(X_test, y_test, transforms=current_transforms)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6cdf7a39-fce4-44d5-9a0e-00397b31025e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "lr = 3e-4\n",
    "log_name = 'iraa'\n",
    "loss_fn = nn.MSELoss()\n",
    "print(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7ec9f-48e3-4f2f-9853-02bbb3f89357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80461a-dc2c-468a-acbb-c274f7a0ed4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_val_corr = -1000\n",
    "for epoch in range(num_epochs):\n",
    "    model = model.to(device)\n",
    "    ############### TRAIN #####################\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    all_pred_scores = []\n",
    "    all_true_scores = []\n",
    "    for batch in tqdm(train_dataloader, total=len(train_dataloader), desc=f\"epoch: {str(epoch).zfill(3)} | train\"):\n",
    "        optimizer.zero_grad()\n",
    "        batch_images, batch_labels = batch \n",
    "        pred_scores = model(batch_images.to(device))\n",
    "        pred_scores = (pred_scores - _mean) / _std\n",
    "        loss = loss_fn(pred_scores, batch_labels.unsqueeze(1).float().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred_scores = pred_scores.detach().cpu().flatten()\n",
    "\n",
    "        all_pred_scores.extend(pred_scores.numpy())\n",
    "        all_true_scores.extend(batch_labels.numpy())\n",
    "\n",
    "        losses.append(loss.detach().cpu().item())\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "    all_pred_scores = np.array(all_pred_scores).flatten()\n",
    "    all_true_scores = np.array(all_true_scores).flatten()\n",
    "    train_loss = np.mean(losses)\n",
    "    train_corr = np.corrcoef(all_pred_scores, all_true_scores)[0, 1]\n",
    "    \n",
    "    ############### EVAL ######################\n",
    "    model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    all_pred_scores = []\n",
    "    all_true_scores = []\n",
    "    for batch in tqdm(val_dataloader, total=len(val_dataloader), desc=f\"epoch: {str(epoch).zfill(3)} | eval\"):\n",
    "        batch_images, batch_labels = batch\n",
    "        pred_scores = model(batch_images.to(device))\n",
    "        pred_scores = (pred_scores - _mean) / _std\n",
    "        pred_scores = pred_scores.detach().cpu()\n",
    "\n",
    "        loss = loss_fn(pred_scores, batch_labels.unsqueeze(1).float())\n",
    "        losses.append(loss.detach().cpu().item())\n",
    "\n",
    "        all_pred_scores.extend(pred_scores.numpy())\n",
    "        all_true_scores.extend(batch_labels.numpy())\n",
    "        \n",
    "    all_pred_scores = np.array(all_pred_scores).flatten()\n",
    "    all_true_scores = np.array(all_true_scores).flatten()\n",
    "    val_loss = np.mean(losses)\n",
    "    val_corr = np.corrcoef(all_pred_scores, all_true_scores)[0, 1]\n",
    "    \n",
    "    with open(join_path('/home/jovyan/work/experiments/logs', f'{log_name}.log'), 'a') as f:\n",
    "        f.write(f\"{epoch},{train_loss},{train_corr},{val_loss},{val_corr}\\n\")\n",
    "    \n",
    "    if val_corr > best_val_corr:\n",
    "        best_val_corr = val_corr\n",
    "        torch.save(model.state_dict(), join_path('/home/jovyan/work/experiments/logs', f'{log_name}_best.pt'))\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    print(f\"epoch: {str(epoch).zfill(3)} | train_loss: {train_loss:5.3f} | train_corr: {train_corr:5.3f} | val_loss: {val_loss:5.3f} | val_corr: {val_corr:5.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c23ea-a6a9-4688-9350-0577ee8f2636",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IRAA(dropout=0.1, pool_kernel_size=3, average_pool_size=3)\n",
    "save_model_path = '/home/jovyan/work/experiments/models/IRAA/model_padding.pt'\n",
    "model.load_state_dict(torch.load(save_model_path))\n",
    "all_true_scores, all_pred_scores = evaluate(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2ae8f791-62ed-4b18-a08f-bc48bb48e2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [10:16<00:00,  3.28s/it]\n"
     ]
    }
   ],
   "source": [
    "all_true_scores, all_pred_scores = evaluate(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8d95c-4193-47ab-a107-13eb9a0ef0ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(np.corrcoef(all_pred_scores, all_true_scores)[0, 1])\n",
    "print(mean_squared_error(all_pred_scores, all_true_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783b25f-160e-43d7-8d77-6696c6133e35",
   "metadata": {},
   "source": [
    "# Eval Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b60986e8-d452-4d76-bf47-8ead56c58d05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_padding_no_target(batch):\n",
    "    max_h = max([b.shape[1] for b in batch])\n",
    "    max_w = max([b.shape[2] for b in batch])\n",
    "    h = 2 ** math.ceil(np.log2(max_h))\n",
    "    w = 2 ** math.ceil(np.log2(max_w))\n",
    "    imgs = torch.zeros(len(batch), 3, h, w)\n",
    "    for i, im in enumerate(batch):\n",
    "        new_w = int((w - im.shape[2]) / 2)\n",
    "        new_h = int((h - im.shape[1]) / 2)\n",
    "        imgs[i] = pad(im, pad=(new_w, w - new_w - im.shape[2], new_h, h - new_h - im.shape[1]), value=0)\n",
    "    return imgs\n",
    "\n",
    "def collate_padding_new_no_target(batch):\n",
    "    max_h = max([b.shape[1] for b in batch])\n",
    "    max_w = max([b.shape[2] for b in batch])\n",
    "    h = max_h\n",
    "    w = max_w\n",
    "    imgs = torch.zeros(len(batch), 3, h, w)\n",
    "\n",
    "    for i, (im) in enumerate(batch):\n",
    "        new_w = int((w - im.shape[2]) / 2)\n",
    "        new_h = int((h - im.shape[1]) / 2)\n",
    "        imgs[i] = pad(im, pad=(new_w, w - new_w - im.shape[2], new_h, h - new_h - im.shape[1]), value=0)\n",
    "    return imgs\n",
    "\n",
    "def predict_only(model, dataloader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_pred_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch_images in tqdm(dataloader, total=len(dataloader), desc=\"eval\"):\n",
    "            pred_scores = model(batch_images.to(device)).detach().cpu()\n",
    "            all_pred_scores.extend(pred_scores.numpy())\n",
    "    all_pred_scores = np.array(all_pred_scores).flatten()\n",
    "    return all_pred_scores\n",
    "\n",
    "class ImageGainDataset(Dataset):\n",
    "    def __init__(self, images: list,  transforms=None):\n",
    "        self.images = images\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.images[idx]\n",
    "        image = Image.open(filename).convert('RGB')\n",
    "        image_np = np.array(image)\n",
    "        if len(image_np.shape) == 2:\n",
    "            image_np = image_np[:, :, None]\n",
    "            image_np = np.concatenate((image_np, image_np, image_np), axis=2)\n",
    "            image = Image.fromarray(image_np)            \n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab2abdf5-80f5-43e1-9904-3e47d9c96d43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_TO_FOLDER = '/home/jovyan/work/data'\n",
    "batch_size = 8\n",
    "\n",
    "all_names = {\n",
    "    'nips2017': join_path(PATH_TO_FOLDER, 'NIPS2017/images', '*'),\n",
    "    'kadid': join_path(PATH_TO_FOLDER, 'KADID10K/kadid10k/images', '*'),\n",
    "    'liv_in_wild': '/home/jovyan/storage/datasets/subjective/image/LIVE_itW/Images/*.*',\n",
    "    'tid2013': join_path(PATH_TO_FOLDER, 'TID2013/distorted_images', '*'), \n",
    "    'cid2013': join_path(PATH_TO_FOLDER, 'CID2013', \"*\", \"*\", \"*.jpg\"),\n",
    "    'pipal': join_path(PATH_TO_FOLDER, 'PIPAL_train', \"*\", \"*.bmp\"),\n",
    "    'test2017': join_path(PATH_TO_FOLDER, 'test2017', '*'),\n",
    "    'div2k': join_path(PATH_TO_FOLDER, 'DIV2K_train_HR', '*.png'), \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399f9e5-d62f-44c9-b962-fd88d1c8df7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = IRAA(dropout=0.1, pool_kernel_size=3, average_pool_size=3)\n",
    "save_model_path = '/home/jovyan/work/experiments/models/IRAA/model_padding.pt'\n",
    "model.load_state_dict(torch.load(save_model_path))\n",
    "model_name = 'iraa'\n",
    "\n",
    "for name, im_dir in all_names.items():\n",
    "    batch_size = 8\n",
    "    if name in ['nips2017', 'kadid']:\n",
    "        continue\n",
    "    if name == 'div2k':\n",
    "        batch_size = 4\n",
    "    print(name)\n",
    "    X = sorted(glob(im_dir))\n",
    "    dataset = ImageGainDataset(X, transforms=transforms_super_simple)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=1, shuffle=False, collate_fn=collate_padding_new_no_target)\n",
    "    pred_scores = predict_only(model, dataloader, device)\n",
    "    \n",
    "    save_path = f'/home/jovyan/work/experiments/logs/{model_name}_dataset_hack_{name}.npy'\n",
    "    with open(save_path, 'wb') as f:\n",
    "        np.save(f, pred_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc6ecd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
