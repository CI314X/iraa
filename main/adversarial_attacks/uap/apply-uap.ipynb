{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install sk-video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aIhjLR2OoKwN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import torch\n",
    "import pandas as pd\n",
    "import gc\n",
    "from os.path import join as join_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_TO_FOLDER = '/home/jovyan/work/'\n",
    "path = join_path(PATH_TO_FOLDER, 'run_attacks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1678645276679,
     "user": {
      "displayName": "Даниил Константинов",
      "userId": "01997835485140301621"
     },
     "user_tz": -180
    },
    "id": "F5hvIQhhDib_",
    "outputId": "e41c57f0-3b6c-444b-cc57-553db20267b5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "DEVICE = device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BJkjGuL64iM",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Example of attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "RLytS6pO6ioQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# noise_file = f'{path}/uap/perturbations/lin_n.png'\n",
    "# universal_noise = cv2.imread(noise_file)\n",
    "# universal_noise = cv2.cvtColor(universal_noise, cv2.COLOR_BGR2RGB).astype('float32')\n",
    "# universal_noise /= 255.\n",
    "# universal_noise -= 0.5\n",
    "\n",
    "# image_file = '/content/drive/MyDrive/colab/iqa_aimasters/data/val2017/000000001268.jpg'\n",
    "# eps = 20 / 255\n",
    "# image = cv2.imread(image_file)\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('float32')\n",
    "# image /= 255.\n",
    "# h, w = image.shape[0], image.shape[1]\n",
    "\n",
    "# universal_add = np.tile(universal_noise, (h//256 + 1, w//256 + 1, 1))[:h, :w, :]\n",
    "# universal_add[universal_add > eps] = eps\n",
    "# universal_add[universal_add < -eps] = -eps\n",
    "# image_after_attack = universal_add + image\n",
    "# image_after_attack[image_after_attack > 1] = 1\n",
    "# image_after_attack[image_after_attack < 0] = 0\n",
    "# image_after_attack = (image_after_attack * 255).astype('uint8')\n",
    "# image_after_attack = cv2.cvtColor(image_after_attack, cv2.COLOR_RGB2BGR)\n",
    "# cv2.imwrite(f'{path}/result.png', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3pQc5eElocK"
   },
   "source": [
    "# Run IQA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18804,
     "status": "ok",
     "timestamp": 1678645252172,
     "user": {
      "displayName": "Даниил Константинов",
      "userId": "01997835485140301621"
     },
     "user_tz": -180
    },
    "id": "uU2FJIwDu37z",
    "outputId": "19f2a011-dbff-4718-f8f2-bcda316bc0a9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10206"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _type_folder = 'TID2013/distorted_images' # val2017 train2017 test2017 DIV2K_train_HR\n",
    "_type_folder = 'KADID10K/kadid10k/images'\n",
    "all_images = sorted(glob.glob(join_path(PATH_TO_FOLDER, 'data', _type_folder, '*')))\n",
    "# image_type_folder = f'mscoco_{_type_folder}'\n",
    "# image_type_folder = 'tid2013'\n",
    "image_type_folder = 'kadid10k'\n",
    "# image_type_folder = _type_folder\n",
    "\n",
    "len(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alltype_folder = 'TID2013/distorted_images' # val2017 train2017 test2017\n",
    "_type_folder = 'train2017'\n",
    "\n",
    "# _type_folder = 'KADID10K/kadid10k/images'\n",
    "# image_type_folder = 'kadid10k'\n",
    "\n",
    "all_images = [join_path(PATH_TO_FOLDER, 'data', _type_folder, '000000009898.jpg'), join_path(PATH_TO_FOLDER, 'data', _type_folder, '000000333018.jpg')]\n",
    "image_type_folder = f'mscoco_{_type_folder}'\n",
    "# image_type_folder = _type_folder\n",
    "# image_type_folder = 'tid2013'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(join_path(PATH_TO_FOLDER, 'data', 'KADID10K', 'images_HackPredictModelV31_padding.csv'))\n",
    "# df['distortion_type'] = df['image_name'].apply(lambda x: int(x.split('_')[1]) if len(x) != 7 else 0)\n",
    "# ims = list(set(df[df['distortion_type'] == 6]['image_name'])) + list(set(df[df['distortion_type'] == 22]['image_name']))\n",
    "# all_images = [join_path(PATH_TO_FOLDER, 'data', 'KADID10K', 'kadid10k', 'images', i) for i in ims]\n",
    "# all_images = sorted(all_images)\n",
    "\n",
    "# image_type_folder = 'kadid10k_only_6_22'\n",
    "\n",
    "# len(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egsXTvs2lvg0",
    "tags": []
   },
   "source": [
    "## Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L2sF6o-PlyN5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import resize, to_tensor, normalize\n",
    "from PIL import Image\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wQdLN5U_6mU8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def attack_image(image_file, universal_noise, eps=20./255.):\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('float32')\n",
    "    image /= 255.\n",
    "    h, w = image.shape[0], image.shape[1]\n",
    "    universal_add = np.tile(universal_noise, (h//256 + 1, w//256 + 1, 1))[:h, :w, :]\n",
    "    universal_add[universal_add > eps] = eps\n",
    "    universal_add[universal_add < -eps] = -eps\n",
    "\n",
    "    image_after_attack = universal_add + image\n",
    "    image_after_attack[image_after_attack > 1] = 1\n",
    "    image_after_attack[image_after_attack < 0] = 0\n",
    "    image_after_attack = (image_after_attack * 255).astype('uint8')\n",
    "    image_after_attack = cv2.cvtColor(image_after_attack, cv2.COLOR_RGB2BGR)\n",
    "    return image_after_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EegPYDnsl3x6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(join_path(PATH_TO_FOLDER, 'metrics', 'Linearity'))\n",
    "from IQAmodel import IQAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPvG2eNpl8Xp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = IQAModel().to(device)\n",
    "model_path = '/home/jovyan/storage/NR-metric-models/p1q2.pth'\n",
    "checkpoint = torch.load(model_path)\n",
    "k = checkpoint['k']\n",
    "b = checkpoint['b']\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1677619233282,
     "user": {
      "displayName": "Даниил Константинов",
      "userId": "01997835485140301621"
     },
     "user_tz": -180
    },
    "id": "8L9LEoxjmJER",
    "outputId": "379c6831-63f5-4c9d-f58a-4957bc07b935",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_file = join_path(path, 'uap', 'perturbations', 'lin_n.png')\n",
    "universal_noise = cv2.imread(noise_file)\n",
    "universal_noise = cv2.cvtColor(universal_noise, cv2.COLOR_BGR2RGB).astype('float32')\n",
    "universal_noise /= 255.\n",
    "universal_noise -= 0.5\n",
    "universal_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps = 20 / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uap_20_255_linearity_mscoco_train2017.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = f'uap_{int(eps * 255)}_255_linearity_{image_type_folder}.csv'\n",
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1077499,
     "status": "ok",
     "timestamp": 1677617779022,
     "user": {
      "displayName": "Даниил Константинов",
      "userId": "01997835485140301621"
     },
     "user_tz": -180
    },
    "id": "vFvaLTqopMtu",
    "outputId": "7095a9c4-19c5-4fcc-fe19-08b7c9b761fc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "res = []\n",
    "for image_file in tqdm(all_images[:1]):\n",
    "    im = Image.open(image_file).convert('RGB')\n",
    "    im = resize(im, (498, 664))\n",
    "    im = to_tensor(im).to(device)\n",
    "    im = normalize(im, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "    with torch.no_grad():\n",
    "      y = model(im.unsqueeze(0))[-1].cpu().detach().item()\n",
    "      before_score = y * k[-1] + b[-1]\n",
    "\n",
    "    image_after_attack = attack_image(image_file, universal_noise)\n",
    "    im_pil = Image.fromarray(cv2.cvtColor(image_after_attack, cv2.COLOR_BGR2RGB))\n",
    "    im = im_pil.convert('RGB')\n",
    "    im = resize(im, (498, 664))\n",
    "    im = to_tensor(im).to(device)\n",
    "    im = normalize(im, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "    with torch.no_grad():\n",
    "        y = model(im.unsqueeze(0))[-1].cpu().detach().item()\n",
    "        after_score = y * k[-1] + b[-1]\n",
    "    res.append([image_file.split('/')[-1], before_score, after_score])\n",
    "\n",
    "df = pd.DataFrame(res, columns =['image_name', 'before_score', 'after_score'])\n",
    "df['metric'] = 'lin'\n",
    "df['attack_type'] = f'uap_{int(eps * 255)}_255'\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSDjjWOjv3--",
    "tags": []
   },
   "source": [
    "## Paq-2-Piq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1677619224241,
     "user": {
      "displayName": "Даниил Константинов",
      "userId": "01997835485140301621"
     },
     "user_tz": -180
    },
    "id": "G8sKcHGxv5TN",
    "outputId": "3964433e-df9b-4cac-b602-fef767e50460",
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise_file = join_path(path, 'uap', 'perturbations', 'p2p_n.png')\n",
    "universal_noise = cv2.imread(noise_file)\n",
    "universal_noise = cv2.cvtColor(universal_noise, cv2.COLOR_BGR2RGB).astype('float32')\n",
    "universal_noise /= 255.\n",
    "universal_noise -= 0.5\n",
    "universal_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 621,
     "status": "ok",
     "timestamp": 1677617881874,
     "user": {
      "displayName": "Даниил Константинов",
      "userId": "01997835485140301621"
     },
     "user_tz": -180
    },
    "id": "-jbQZwUtv7cc",
    "outputId": "43f44962-1472-43ea-976d-520eb19fada9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['my_device'] = str(device)\n",
    "\n",
    "os.chdir(join_path(PATH_TO_FOLDER, 'metrics', 'Paq-2-Piq'))\n",
    "from paq2piq_standalone import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = join_path(PATH_TO_FOLDER, 'metrics', 'Paq-2-Piq', 'RoIPoolModel.pth')\n",
    "model = InferenceModel(RoIPoolModel().to(device), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_v407QJweKZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps = 10.0 / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 262916,
     "status": "ok",
     "timestamp": 1677618282653,
     "user": {
      "displayName": "Даниил Константинов",
      "userId": "01997835485140301621"
     },
     "user_tz": -180
    },
    "id": "XKBO4LwVwSKm",
    "outputId": "0c490446-0f56-4270-8fec-26c352bf11c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "res = []\n",
    "gains = []\n",
    "for image_file in tqdm(all_images): # all_images\n",
    "    im = cv2.imread(image_file)\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    before_score = model.predict(im)['global_score'] # or score = model.predict_from_file(path_to_img)\n",
    "\n",
    "    image = im.astype('float32')\n",
    "    image /= 255.\n",
    "    h, w = image.shape[0], image.shape[1]\n",
    "    universal_add = np.tile(universal_noise, (h//256 + 1, w//256 + 1, 1))[:h, :w, :]\n",
    "    universal_add[universal_add > eps] = eps\n",
    "    universal_add[universal_add < -eps] = -eps\n",
    "\n",
    "    image_after_attack = universal_add + image\n",
    "    image_after_attack[image_after_attack > 1] = 1\n",
    "    image_after_attack[image_after_attack < 0] = 0\n",
    "    image_after_attack = (image_after_attack * 255).astype('uint8')\n",
    "    after_score = model.predict(image_after_attack)['global_score']\n",
    "    gains.append([before_score, after_score])\n",
    "    \n",
    "    res.append([image_file.split('/')[-1], before_score, after_score])\n",
    "gains = np.array(gains)\n",
    "\n",
    "df = pd.DataFrame(res, columns=['image_name', 'before_score', 'after_score'])\n",
    "df['attack_type'] = f'uap_{int(eps * 255)}_255'\n",
    "df['metric'] = 'p2p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3Aoflsw7ONA",
    "tags": []
   },
   "source": [
    "## SPAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1677619829560,
     "user": {
      "displayName": "Даниил Константинов",
      "userId": "01997835485140301621"
     },
     "user_tz": -180
    },
    "id": "EThFy5I26bpB",
    "outputId": "f1f06aa4-c6da-4a96-db1a-e5c62c312208",
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise_file = join_path(path, 'uap', 'perturbations', 'spaq_n.png')\n",
    "\n",
    "universal_noise = cv2.imread(noise_file)\n",
    "universal_noise = cv2.cvtColor(universal_noise, cv2.COLOR_BGR2RGB).astype('float32')\n",
    "universal_noise /= 255.\n",
    "universal_noise -= 0.5\n",
    "universal_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0VbzDeJ7PZY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(join_path(PATH_TO_FOLDER, 'metrics', 'SPAQ'))\n",
    "from Prepare_image import Image_load\n",
    "\n",
    "class Baseline(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Baseline, self).__init__()\n",
    "\t\tself.backbone = torchvision.models.resnet50(pretrained=False)\n",
    "\t\tfc_feature = self.backbone.fc.in_features\n",
    "\t\tself.backbone.fc = torch.nn.Linear(fc_feature, 1, bias=True)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tresult = self.backbone(x)\n",
    "\t\treturn result\n",
    "\n",
    "model = Baseline()\n",
    "\n",
    "model_path = '/home/jovyan/storage/NR-metric-models/BL_release.pt'\n",
    "checkpoint = torch.load(model_path, device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(device);\n",
    "model.eval();\n",
    "\n",
    "prepare_image = Image_load(size=512, stride=224)\n",
    "\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1677621867351,
     "user": {
      "displayName": "Даниил Константинов",
      "userId": "01997835485140301621"
     },
     "user_tz": -180
    },
    "id": "YdIc6Wep8Dxn",
    "outputId": "90ba54c9-9fc1-4fe0-db60-f6daf66484bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps = 20 / 255\n",
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 702182,
     "status": "ok",
     "timestamp": 1677622573641,
     "user": {
      "displayName": "Даниил Константинов",
      "userId": "01997835485140301621"
     },
     "user_tz": -180
    },
    "id": "k91285eK713i",
    "outputId": "cce9abfb-c33e-405a-8301-0914072cebd5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "res = []\n",
    "for image_file in tqdm(all_images):\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image.astype('float32')\n",
    "    try:\n",
    "        img = prepare_image(Image.open(image_file).convert(\"RGB\")).to(device)\n",
    "    except:\n",
    "        res.append([image_file.split('/')[-1], before_score, after_score])\n",
    "        continue\n",
    "\n",
    "    with torch.no_grad():\n",
    "        before_score = model(img).mean().detach().cpu().item()\n",
    "\n",
    "    image /= 255.\n",
    "    h, w = image.shape[0], image.shape[1]\n",
    "    universal_add = np.tile(universal_noise, (h//224 + 1, w//224 + 1, 1))[:h, :w, :]\n",
    "    universal_add[universal_add > eps] = eps\n",
    "    universal_add[universal_add < -eps] = -eps\n",
    "    image_after_attack = universal_add + image\n",
    "    image_after_attack[image_after_attack > 1] = 1\n",
    "    image_after_attack[image_after_attack < 0] = 0\n",
    "    image_after_attack = (image_after_attack * 255).astype('uint8')\n",
    "    img = prepare_image(Image.fromarray(image_after_attack).convert(\"RGB\")).to(device)\n",
    "    # img = to_tensor(Image.fromarray(image_after_attack)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        after_score = model(img).mean().detach().cpu().item()\n",
    "\n",
    "    res.append([image_file.split('/')[-1], before_score, after_score])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(res, columns=['image_name', 'before_score', 'after_score'])\n",
    "df['attack_type'] = f'uap_{int(eps * 255)}_255'\n",
    "df['metric'] = 'spaq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7zwfXNg2NGo",
    "tags": []
   },
   "source": [
    "## KonCept512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QF7beKMLDaU0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import os\n",
    "from torchvision.transforms import ToTensor, Resize, Normalize\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1010,
     "status": "ok",
     "timestamp": 1678645318461,
     "user": {
      "displayName": "Даниил Константинов",
      "userId": "01997835485140301621"
     },
     "user_tz": -180
    },
    "id": "ry9yCMpe3__1",
    "outputId": "e6be63fb-6864-4e00-ccc4-c10fefa9a621",
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise_file = join_path(path, 'uap', 'perturbations', 'koncept_n.png')\n",
    "\n",
    "universal_noise = cv2.imread(noise_file)\n",
    "universal_noise = cv2.cvtColor(universal_noise, cv2.COLOR_BGR2RGB).astype('float32')\n",
    "universal_noise /= 255.\n",
    "universal_noise -= 0.5\n",
    "universal_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPVKlwwI2Nxm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(join_path(PATH_TO_FOLDER, 'metrics', 'KonCept512'))\n",
    "\n",
    "from inceptionresnetv2 import inceptionresnetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8J1dd_L82LnE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class model_qa(nn.Module):\n",
    "    def __init__(self,num_classes,**kwargs):\n",
    "        super(model_qa,self).__init__()\n",
    "        base_model = inceptionresnetv2(num_classes=1000, pretrained='imagenet')\n",
    "        self.base = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1536, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256),         \n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.base(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4eRmaX92h1w",
    "tags": []
   },
   "outputs": [],
   "source": [
    "KonCept512 = model_qa(num_classes=1)\n",
    "KonCept512.load_state_dict(torch.load('/home/jovyan/storage/NR-metric-models/KonCept512.pth'))\n",
    "KonCept512.eval().to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTdCen-92h4D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "koncept_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((384, 512)),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QujkC01YEVSd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def img_to_batch_koncept(img):\n",
    "    img_batch = torch.zeros(1, 3, 384, 512).to(device)  \n",
    "    img_batch[0]  = koncept_transform(img) \n",
    "    return img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1678645387259,
     "user": {
      "displayName": "Даниил Константинов",
      "userId": "01997835485140301621"
     },
     "user_tz": -180
    },
    "id": "0CnOpVip3iGg",
    "outputId": "f08b6d0b-1123-430f-fbee-db119393ca91",
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps = 26 / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1794426,
     "status": "ok",
     "timestamp": 1678648284942,
     "user": {
      "displayName": "Даниил Константинов",
      "userId": "01997835485140301621"
     },
     "user_tz": -180
    },
    "id": "ZAJMJECD3iJG",
    "outputId": "3e8c7ba0-926d-4cc0-db80-d266abd91530",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "res = []\n",
    "gains = []\n",
    "for image_file in tqdm(all_images):\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    with torch.no_grad():\n",
    "        before_score = KonCept512(img_to_batch_koncept(image).to(device)).detach().detach().cpu().numpy()[0][0]\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.\n",
    "    h, w = image.shape[0], image.shape[1]\n",
    "    universal_add = np.tile(universal_noise, (h//300 + 1, w//300 + 1, 1))[:h, :w, :]\n",
    "    universal_add[universal_add > eps] = eps\n",
    "    universal_add[universal_add < -eps] = -eps\n",
    "    image_after_attack = universal_add + image\n",
    "    image_after_attack[image_after_attack > 1] = 1\n",
    "    image_after_attack[image_after_attack < 0] = 0\n",
    "    image_after_attack = (image_after_attack * 255).astype('uint8')\n",
    "    with torch.no_grad():\n",
    "        after_score = KonCept512(img_to_batch_koncept(image_after_attack).to(device)).detach().detach().cpu().numpy()[0][0]\n",
    "    \n",
    "    gains.append([before_score, after_score])\n",
    "    res.append([image_file.split('/')[-1], before_score, after_score])\n",
    "gains = np.array(gains)\n",
    "\n",
    "df = pd.DataFrame(res, columns=['image_name', 'before_score', 'after_score'])\n",
    "df['attack_type'] = f'uap_{int(eps * 255)}_255'\n",
    "df['metric'] = 'koncept'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDTVSFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def attack_image(image_file, universal_noise, eps=20./255.):\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('float32')\n",
    "    image /= 255.\n",
    "    h, w = image.shape[0], image.shape[1]\n",
    "    universal_add = np.tile(universal_noise, (h//256 + 1, w//256 + 1, 1))[:h, :w, :]\n",
    "    universal_add[universal_add > eps] = eps\n",
    "    universal_add[universal_add < -eps] = -eps\n",
    "\n",
    "    image_after_attack = universal_add + image\n",
    "    image_after_attack[image_after_attack > 1] = 1\n",
    "    image_after_attack[image_after_attack < 0] = 0\n",
    "    image_after_attack = (image_after_attack * 255).astype('uint8')\n",
    "    image_after_attack = cv2.cvtColor(image_after_attack, cv2.COLOR_RGB2BGR)\n",
    "    return image_after_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise_file = join_path(path, 'uap', 'perturbations', 'mdtvsfa_n.png')\n",
    "universal_noise = cv2.imread(noise_file)\n",
    "universal_noise = cv2.cvtColor(universal_noise, cv2.COLOR_BGR2RGB).astype('float32')\n",
    "universal_noise /= 255.\n",
    "universal_noise -= 0.5\n",
    "universal_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torchvision import transforms\n",
    "import skvideo.io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.utils.backcompat.broadcast_warning.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(join_path(PATH_TO_FOLDER, 'metrics', 'MDTVSFA'))\n",
    "\n",
    "from VQAmodel import VQAModel\n",
    "from CNNfeatures import CNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = VQAModel().to(device)\n",
    "model.load_state_dict(torch.load('/home/jovyan/storage/NR-metric-models/MDTVSFA.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get features\n",
    "extractor = CNNModel(model='ResNet-50').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train();\n",
    "extractor.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps = 9 / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "gains = []\n",
    "i = 0\n",
    "for image_file in tqdm(all_images):\n",
    "    image = np.array(Image.open(image_file).convert('RGB'))\n",
    "    with torch.no_grad():\n",
    "        transformed_video = transform(image).unsqueeze(0)\n",
    "        features_mean, features_std = extractor(transformed_video.to(device))\n",
    "        features = torch.cat((features_mean, features_std), 1).squeeze()\n",
    "        features = torch.unsqueeze(features, 0)\n",
    "        input_length = features.shape[1] * torch.ones(1, 1, dtype=torch.long)\n",
    "\n",
    "        before_score = model((features, input_length))\n",
    "        before_score = before_score[0].detach().cpu().item()\n",
    "        \n",
    "    \n",
    "    image_after_attack = attack_image(image_file, universal_noise, eps=eps)\n",
    "    image_after_attack = Image.fromarray(cv2.cvtColor(image_after_attack, cv2.COLOR_BGR2RGB))\n",
    "    with torch.no_grad():\n",
    "        transformed_video = transform(image_after_attack).unsqueeze(0)\n",
    "        features_mean, features_std = extractor(transformed_video.to(device))\n",
    "        features = torch.cat((features_mean, features_std), 1).squeeze()\n",
    "        features = torch.unsqueeze(features, 0)\n",
    "        input_length = features.shape[1] * torch.ones(1, 1, dtype=torch.long)\n",
    "        \n",
    "        after_score = model((features, input_length))\n",
    "        after_score = after_score[0].detach().cpu().item()\n",
    "        \n",
    "    gains.append([before_score, after_score])\n",
    "    res.append([image_file.split('/')[-1], before_score, after_score])\n",
    "\n",
    "gains = np.array(gains)\n",
    "\n",
    "df = pd.DataFrame(res, columns=['image_name', 'before_score', 'after_score'])\n",
    "df['attack_type'] = f'uap_{int(eps * 255)}_255'\n",
    "df['metric'] = 'mdtvsfa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
